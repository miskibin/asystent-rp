{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the capital of France?\",\n",
    "answer=\"The capital of France is Paris.\",\n",
    "ground_truth=\"Paris is the capital and most populous city of France.\"\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma2:latest\", temperature=0.0)  # You can change the model as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': '1. **Correctness:** The criteria states that the submission should be correct, accurate, and factual. \\n2. **Comparison:** We need to compare the submission (\"The answer is 4\") with the reference (\"There are 3 apples\").\\n3. **Conclusion on Correctness:**  The submission and the reference provide different answers. Therefore, the submission is not correct.\\n\\n\\nN',\n",
       " 'value': 'N',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = LabeledCriteriaEvalChain.from_llm(llm=llm, criteria=\"correctness\")\n",
    "response = evaluator.evaluate_strings(\n",
    "prediction=\"The answer is 4\",\n",
    "input=\"How many apples are there?\",\n",
    "reference=\"There are 3 apples\",\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Step 1: Identify the criterion being evaluated - correctness.\\n'\n",
      "              'Step 2: Determine what constitutes \"correctness\" in this '\n",
      "              'context - it means that the submitted response must match the '\n",
      "              'factual information provided in the reference data.\\n'\n",
      "              'Step 3: Compare the submission to the reference data:\\n'\n",
      "              '   - Submission states there are 4 apples.\\n'\n",
      "              '   - Reference data states there are 3 apples.\\n'\n",
      "              'Step 4: Evaluate if the submission matches the reference data:\\n'\n",
      "              '   - Since the submission states a different number of apples '\n",
      "              'than what is stated in the reference, it does not match.\\n'\n",
      "              'Conclusion: The submission does not meet the correctness '\n",
      "              'criterion as it does not accurately state the number of apples '\n",
      "              'provided in the reference.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Criteria evaluation\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# criteria_evaluator = CriteriaEvalChain.from_llm(\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     llm=ollama_llm,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     criteria=[\"correctness\"],\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m      8\u001b[0m criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mLabeledCriteriaEvalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miskibin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py:583\u001b[0m, in \u001b[0;36mLabeledCriteriaEvalChain.from_llm\u001b[1;34m(cls, llm, criteria, prompt, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CriteriaEvalChain:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a `LabeledCriteriaEvalChain` instance from an llm and criteria.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m     criteria_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_criteria(criteria)\n\u001b[0;32m    585\u001b[0m     criteria_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m criteria_\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\miskibin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py:528\u001b[0m, in \u001b[0;36mLabeledCriteriaEvalChain._resolve_prompt\u001b[1;34m(cls, prompt)\u001b[0m\n\u001b[0;32m    526\u001b[0m expected_input_vars \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriteria\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    527\u001b[0m prompt_ \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m PROMPT_WITH_REFERENCES\n\u001b[1;32m--> 528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_input_vars \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mprompt_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m):\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput variables should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'input_variables'"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Criteria evaluation\n",
    "# criteria_evaluator = CriteriaEvalChain.from_llm(\n",
    "#     llm=ollama_llm,\n",
    "#     criteria=[\"correctness\"],\n",
    "# )\n",
    "criteria = \"correctness\"\n",
    "evaluator = LabeledCriteriaEvalChain.from_llm(\n",
    "    llm=ollama_llm,\n",
    "    criteria=criteria,\n",
    "    prompt=prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluator.evaluate_strings(\n",
    "  prediction=\"The  answer is 4\",\n",
    "  input=\"How many apples are there?\",\n",
    "  reference=\"There are 3 apples\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'To assess the submission against the criteria, I will break it '\n",
      "              'down step by step:\\n'\n",
      "              '\\n'\n",
      "              '1. Correctness: To determine if the submission is correct, I '\n",
      "              'need to verify if the answer provided (4) matches the actual '\n",
      "              'number of apples (3). Since 4 does not equal 3, the submission '\n",
      "              'does not meet this criterion.\\n'\n",
      "              '\\n'\n",
      "              '2. Accuracy: The submission claims that the answer is indeed 4, '\n",
      "              'which implies that it believes its own response is accurate. '\n",
      "              \"However, as established in step 1, the submission's claim is \"\n",
      "              'incorrect because there are actually 3 apples. Therefore, the '\n",
      "              'submission does not meet this criterion either.\\n'\n",
      "              '\\n'\n",
      "              '3. Factual accuracy: This criterion requires that the '\n",
      "              'submission provide a factual answer based on actual '\n",
      "              'information. Since the submission provides an incorrect number '\n",
      "              '(4) and does not acknowledge the correct number of apples (3), '\n",
      "              'it fails to meet this criterion as well.\\n'\n",
      "              '\\n'\n",
      "              'Based on these assessments, I conclude that the submission does '\n",
      "              'not meet all criteria.\\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "source": [
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reasoning', 'Here\\'s the step-by-step reasoning for each criterion:\\n\\n1. Accuracy: \\n   - The ground truth is \"The capital of France is Paris.\" This matches what the submission provides.\\n   - The answer contains a statement that implies it knows the ground truth, which may not necessarily be accurate in all cases.\\n\\n2. Completeness:\\n   - The question asks for the capital of France. The submission addresses this aspect directly.\\n   - However, the submission does not mention any other aspects of the country (e.g., its population, economy, culture).\\n\\n3. Coherence:\\n   - The answer is structured as a declarative sentence stating that Paris is the capital of France.\\n   - There are no apparent inconsistencies or contradictions within the statement.\\n\\nGiven these observations, the conclusion is: \\nY\\n\\nThe submission partially meets all criteria (accuracy and completeness), but lacks coherence in some respects.')\n",
      "('value', 'The submission partially meets all criteria (accuracy and completeness), but lacks coherence in some respects.')\n",
      "('score', None)\n"
     ]
    }
   ],
   "source": [
    "for item in criteria_result.items():\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
