{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the capital of France?\",\n",
    "answer=\"The capital of France is Paris.\",\n",
    "ground_truth=\"Paris is the capital and most populous city of France.\"\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2:latest\", temperature=0)  # You can change the model as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"To assess the submission against the given criteria, I will break down each criterion step by step:\\n\\n1. math-criterion: Is the output of the llm valid?\\n\\nThe output of the llm is 8. To determine if it's valid, we need to check if the calculation 4 + 3 indeed equals 8.\\n\\nStep-by-step calculation:\\n- Start with the given numbers: 4 and 3\\n- Add 4 and 3 together: 4 + 3 = 7 (not 8)\\n- Since the result of the calculation is not 8, we can conclude that the output of the llm is not valid for this math-criterion.\\n\\n2. Since the first criterion is not met, there's no need to assess the second criterion as the submission does not meet all criteria.\\n\\nY\",\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = {\"math-criterion\": \"Is the output of the llm valid?\"}\n",
    "evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\n",
    "response = evaluator.evaluate_strings(\n",
    "    prediction=\"8\",\n",
    "    input=\"What is 4 + 3\",\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriteriaEvalChain(prompt=PromptTemplate(input_variables=['input', 'output'], partial_variables={'criteria': 'my-custom-criterion: Is the submission the most amazing ever?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.'), llm=OllamaLLM(model='llama3.2:latest', temperature=0.0, _client=<ollama._client.Client object at 0x000001480B8C7760>, _async_client=<ollama._client.AsyncClient object at 0x0000014824822080>), criterion_name='my-custom-criterion')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Criteria evaluation\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# criteria_evaluator = CriteriaEvalChain.from_llm(\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     llm=ollama_llm,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     criteria=[\"correctness\"],\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m      8\u001b[0m criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mLabeledCriteriaEvalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miskibin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py:583\u001b[0m, in \u001b[0;36mLabeledCriteriaEvalChain.from_llm\u001b[1;34m(cls, llm, criteria, prompt, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CriteriaEvalChain:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a `LabeledCriteriaEvalChain` instance from an llm and criteria.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m     criteria_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_criteria(criteria)\n\u001b[0;32m    585\u001b[0m     criteria_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m criteria_\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\miskibin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\evaluation\\criteria\\eval_chain.py:528\u001b[0m, in \u001b[0;36mLabeledCriteriaEvalChain._resolve_prompt\u001b[1;34m(cls, prompt)\u001b[0m\n\u001b[0;32m    526\u001b[0m expected_input_vars \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriteria\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    527\u001b[0m prompt_ \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m PROMPT_WITH_REFERENCES\n\u001b[1;32m--> 528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_input_vars \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mprompt_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m):\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput variables should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'input_variables'"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Criteria evaluation\n",
    "# criteria_evaluator = CriteriaEvalChain.from_llm(\n",
    "#     llm=ollama_llm,\n",
    "#     criteria=[\"correctness\"],\n",
    "# )\n",
    "criteria = \"correctness\"\n",
    "evaluator = LabeledCriteriaEvalChain.from_llm(\n",
    "    llm=ollama_llm,\n",
    "    criteria=criteria,\n",
    "    prompt=prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluator.evaluate_strings(\n",
    "  prediction=\"The  answer is 4\",\n",
    "  input=\"How many apples are there?\",\n",
    "  reference=\"There are 3 apples\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'To assess the submission against the criteria, I will break it '\n",
      "              'down step by step:\\n'\n",
      "              '\\n'\n",
      "              '1. Correctness: To determine if the submission is correct, I '\n",
      "              'need to verify if the answer provided (4) matches the actual '\n",
      "              'number of apples (3). Since 4 does not equal 3, the submission '\n",
      "              'does not meet this criterion.\\n'\n",
      "              '\\n'\n",
      "              '2. Accuracy: The submission claims that the answer is indeed 4, '\n",
      "              'which implies that it believes its own response is accurate. '\n",
      "              \"However, as established in step 1, the submission's claim is \"\n",
      "              'incorrect because there are actually 3 apples. Therefore, the '\n",
      "              'submission does not meet this criterion either.\\n'\n",
      "              '\\n'\n",
      "              '3. Factual accuracy: This criterion requires that the '\n",
      "              'submission provide a factual answer based on actual '\n",
      "              'information. Since the submission provides an incorrect number '\n",
      "              '(4) and does not acknowledge the correct number of apples (3), '\n",
      "              'it fails to meet this criterion as well.\\n'\n",
      "              '\\n'\n",
      "              'Based on these assessments, I conclude that the submission does '\n",
      "              'not meet all criteria.\\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "source": [
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reasoning', 'Here\\'s the step-by-step reasoning for each criterion:\\n\\n1. Accuracy: \\n   - The ground truth is \"The capital of France is Paris.\" This matches what the submission provides.\\n   - The answer contains a statement that implies it knows the ground truth, which may not necessarily be accurate in all cases.\\n\\n2. Completeness:\\n   - The question asks for the capital of France. The submission addresses this aspect directly.\\n   - However, the submission does not mention any other aspects of the country (e.g., its population, economy, culture).\\n\\n3. Coherence:\\n   - The answer is structured as a declarative sentence stating that Paris is the capital of France.\\n   - There are no apparent inconsistencies or contradictions within the statement.\\n\\nGiven these observations, the conclusion is: \\nY\\n\\nThe submission partially meets all criteria (accuracy and completeness), but lacks coherence in some respects.')\n",
      "('value', 'The submission partially meets all criteria (accuracy and completeness), but lacks coherence in some respects.')\n",
      "('score', None)\n"
     ]
    }
   ],
   "source": [
    "for item in criteria_result.items():\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
